{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "678fd12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('student_lifestyle_100k.csv')\n",
    "dataset['Gender'] = dataset['Gender'].astype('category')\n",
    "dataset['Department'] = dataset['Department'].astype('category')\n",
    "dataset['Stress_Level'] = dataset['Stress_Level'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6258b533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufteilen nach True und False in der Spalte Depression\n",
    "depressed = dataset[dataset['Depression']]\n",
    "not_depressed = dataset[~dataset['Depression']]\n",
    "\n",
    "# 10.000 zufällige negative Stichproben ziehen\n",
    "not_depressed_sample = not_depressed.sample(n=10062, random_state=42)\n",
    "\n",
    "# Datensatz zusammensetzen\n",
    "balanced_dataset = pd.concat([depressed, not_depressed_sample])\n",
    "\n",
    "x = balanced_dataset.loc[:, 'Age':'Stress_Level']\n",
    "y = balanced_dataset['Depression']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05e79d7",
   "metadata": {},
   "source": [
    "/\\ Allgemeiner Code\n",
    "\n",
    "\\\\/ Spezieller Code für das Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5ca674dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batches per epoch: 755\n",
      "Validation batches: 252\n",
      "Test batches: 252\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "torch.manual_seed(42)  # set random seed for reproducibility\n",
    "\n",
    "# Spaltennamen nach Typ trennen\n",
    "categorical_cols = x.select_dtypes(include=['category']).columns.tolist()\n",
    "numeric_cols = x.select_dtypes(include=['number', 'bool']).columns.tolist()\n",
    "\n",
    "# Preprocessing-Transformer (numerisch: RobustScaler, kategorisch: OneHotEncoder)\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"numeric\", RobustScaler(), numeric_cols), # StandardScaler() um Data Leakage zu vermeiden?\n",
    "    (\"categorical\", OneHotEncoder(drop='first'), categorical_cols)\n",
    "])\n",
    "\n",
    "x_processed = preprocessor.fit_transform(x)\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(\n",
    "    x_processed, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "x_val, x_test, y_val, y_test = train_test_split(\n",
    "    x_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(TensorDataset(torch.FloatTensor(x_train), torch.LongTensor(y_train.values)), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(torch.FloatTensor(x_val), torch.LongTensor(y_val.values)), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(torch.FloatTensor(x_test), torch.LongTensor(y_test.values)), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training batches per epoch: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "929141cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FlexibleClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Flexible neural network that can be configured with different architectures.\n",
    "    Supports variable depth and layer dimensions.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Number of input features\n",
    "            hidden_dims: List of hidden layer dimensions, e.g., [64, 32]\n",
    "            output_dim: Number of output classes\n",
    "            dropout_rate: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Build layers dynamically\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "08af945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        train_loss += loss.item() * batch_X.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "    \n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, criterion):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in data_loader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            eval_loss += loss.item() * batch_X.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "    \n",
    "    avg_loss = eval_loss / len(data_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=100, lr=0.01, weight_decay=0.0, patience=15, trial=None):\n",
    "    \"\"\"Complete training loop\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer) # Train\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion)  # Validate\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "    \n",
    "        if trial is not None:\n",
    "            trial.report(val_acc, epoch)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "    \n",
    "    return best_val_acc, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f3513b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to optimize.\n",
    "    Returns the best validation accuracy for this hyperparameter configuration.\n",
    "    \"\"\"\n",
    "    # Define the search space\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    hidden_dims = []\n",
    "    for i in range(n_layers):\n",
    "        dim = trial.suggest_categorical(f'hidden_dim_layer_{i}', [32, 64, 128, 256])\n",
    "        hidden_dims.append(dim)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-2, log=True)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)    \n",
    "    \n",
    "    # Training the model with the sampled hyperparameters\n",
    "    model = FlexibleClassifier(input_dim=19,hidden_dims=hidden_dims,output_dim=2,dropout_rate=dropout_rate)        \n",
    "    best_val_acc, best_epoch = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        trial=trial  # For Optuna pruning\n",
    "    )\n",
    "    \n",
    "    return best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b947bfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-26 18:53:11,806]\u001b[0m Using an existing study with name 'depression_classification' instead of creating a new one.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37927a604a4c407395006d8b7c265bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-26 18:54:27,418]\u001b[0m Trial 4 finished with value: 0.6670807453416149 and parameters: {'n_layers': 2, 'hidden_dim_layer_0': 128, 'hidden_dim_layer_1': 32, 'weight_decay': 0.009322600084791376, 'learning_rate': 1.25771374830892e-05, 'dropout_rate': 0.33905192010514534}. Best is trial 4 with value: 0.6670807453416149.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 18:54:40,353]\u001b[0m Trial 5 finished with value: 0.6777639751552795 and parameters: {'n_layers': 2, 'hidden_dim_layer_0': 32, 'hidden_dim_layer_1': 64, 'weight_decay': 0.0009189205220429297, 'learning_rate': 0.018145321339269806, 'dropout_rate': 0.2581649230291549}. Best is trial 5 with value: 0.6777639751552795.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 18:54:51,670]\u001b[0m Trial 6 finished with value: 0.5396273291925466 and parameters: {'n_layers': 2, 'hidden_dim_layer_0': 32, 'hidden_dim_layer_1': 256, 'weight_decay': 0.0013585059936427973, 'learning_rate': 0.05982694591013346, 'dropout_rate': 0.14328000782988892}. Best is trial 5 with value: 0.6777639751552795.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 18:55:11,096]\u001b[0m Trial 7 finished with value: 0.6792546583850931 and parameters: {'n_layers': 1, 'hidden_dim_layer_0': 128, 'weight_decay': 0.0021274932405971406, 'learning_rate': 0.01296232545584006, 'dropout_rate': 0.008055085023159825}. Best is trial 7 with value: 0.6792546583850931.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 18:55:31,626]\u001b[0m Trial 8 finished with value: 0.6837267080745342 and parameters: {'n_layers': 2, 'hidden_dim_layer_0': 256, 'hidden_dim_layer_1': 32, 'weight_decay': 2.4118308458873673e-05, 'learning_rate': 0.001421559681016647, 'dropout_rate': 0.4371125881975561}. Best is trial 8 with value: 0.6837267080745342.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 18:55:41,494]\u001b[0m Trial 9 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-26 18:56:08,415]\u001b[0m Trial 10 finished with value: 0.6809937888198758 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 128, 'hidden_dim_layer_1': 256, 'hidden_dim_layer_2': 64, 'weight_decay': 0.00024509473881020036, 'learning_rate': 0.00010023661473505718, 'dropout_rate': 0.3932689299292271}. Best is trial 8 with value: 0.6837267080745342.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 18:56:18,502]\u001b[0m Trial 11 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-26 18:56:25,268]\u001b[0m Trial 12 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-26 18:56:46,867]\u001b[0m Trial 13 finished with value: 0.68472049689441 and parameters: {'n_layers': 1, 'hidden_dim_layer_0': 64, 'weight_decay': 0.0020496756309192734, 'learning_rate': 0.00048494140508293537, 'dropout_rate': 0.14178385067784788}. Best is trial 13 with value: 0.68472049689441.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 18:57:00,739]\u001b[0m Trial 14 finished with value: 0.6782608695652174 and parameters: {'n_layers': 1, 'hidden_dim_layer_0': 64, 'weight_decay': 0.0002596642641228214, 'learning_rate': 0.0004716775252437193, 'dropout_rate': 0.02199666162482075}. Best is trial 13 with value: 0.68472049689441.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 18:57:18,887]\u001b[0m Trial 15 finished with value: 0.68 and parameters: {'n_layers': 1, 'hidden_dim_layer_0': 256, 'weight_decay': 1.3908679463499324e-05, 'learning_rate': 0.0017304213696493261, 'dropout_rate': 0.1582062444569794}. Best is trial 13 with value: 0.68472049689441.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 18:57:35,235]\u001b[0m Trial 16 finished with value: 0.6804968944099379 and parameters: {'n_layers': 2, 'hidden_dim_layer_0': 256, 'hidden_dim_layer_1': 32, 'weight_decay': 6.780074201950947e-05, 'learning_rate': 0.0011447782866910686, 'dropout_rate': 0.26144233292470775}. Best is trial 13 with value: 0.68472049689441.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 18:57:59,447]\u001b[0m Trial 17 finished with value: 0.6760248447204968 and parameters: {'n_layers': 1, 'hidden_dim_layer_0': 256, 'weight_decay': 1.09877939247734e-05, 'learning_rate': 0.00031746677440868597, 'dropout_rate': 0.09185610106774028}. Best is trial 13 with value: 0.68472049689441.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 18:58:21,643]\u001b[0m Trial 18 finished with value: 0.6867080745341615 and parameters: {'n_layers': 2, 'hidden_dim_layer_0': 64, 'hidden_dim_layer_1': 32, 'weight_decay': 0.0006509108140121721, 'learning_rate': 0.004105962208131281, 'dropout_rate': 0.328606140488908}. Best is trial 18 with value: 0.6867080745341615.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 18:58:40,010]\u001b[0m Trial 19 finished with value: 0.6817391304347826 and parameters: {'n_layers': 2, 'hidden_dim_layer_0': 64, 'hidden_dim_layer_1': 128, 'weight_decay': 0.00046918758076009337, 'learning_rate': 0.0055672729877976265, 'dropout_rate': 0.2968677120234401}. Best is trial 18 with value: 0.6867080745341615.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 18:58:45,631]\u001b[0m Trial 20 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-26 18:59:18,056]\u001b[0m Trial 21 finished with value: 0.6864596273291925 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 64, 'hidden_dim_layer_1': 32, 'hidden_dim_layer_2': 32, 'weight_decay': 0.0005331387048146572, 'learning_rate': 0.004083454038663352, 'dropout_rate': 0.35123177372711367}. Best is trial 18 with value: 0.6867080745341615.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 18:59:55,214]\u001b[0m Trial 22 finished with value: 0.68472049689441 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 64, 'hidden_dim_layer_1': 32, 'hidden_dim_layer_2': 32, 'weight_decay': 0.0005918378291595594, 'learning_rate': 0.0048042304019144215, 'dropout_rate': 0.37557575595964077}. Best is trial 18 with value: 0.6867080745341615.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 19:00:33,493]\u001b[0m Trial 23 finished with value: 0.684223602484472 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 64, 'hidden_dim_layer_1': 32, 'hidden_dim_layer_2': 128, 'weight_decay': 0.00020074620656740933, 'learning_rate': 0.0034460706971955637, 'dropout_rate': 0.3257692225981513}. Best is trial 18 with value: 0.6867080745341615.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 19:00:42,038]\u001b[0m Trial 24 pruned. \u001b[0m\n",
      "\u001b[32m[I 2026-02-26 19:00:58,703]\u001b[0m Trial 25 finished with value: 0.684223602484472 and parameters: {'n_layers': 2, 'hidden_dim_layer_0': 64, 'hidden_dim_layer_1': 32, 'weight_decay': 0.003086877370678349, 'learning_rate': 0.0005348419087205896, 'dropout_rate': 0.20561347714700118}. Best is trial 18 with value: 0.6867080745341615.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 19:01:15,336]\u001b[0m Trial 26 finished with value: 0.684223602484472 and parameters: {'n_layers': 1, 'hidden_dim_layer_0': 64, 'weight_decay': 0.0008590106249289578, 'learning_rate': 0.0028987589086170067, 'dropout_rate': 0.3208108842551296}. Best is trial 18 with value: 0.6867080745341615.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 19:01:29,474]\u001b[0m Trial 27 finished with value: 0.6849689440993789 and parameters: {'n_layers': 2, 'hidden_dim_layer_0': 64, 'hidden_dim_layer_1': 32, 'weight_decay': 0.00013807989977617925, 'learning_rate': 0.00734347386016645, 'dropout_rate': 0.057482305402748554}. Best is trial 18 with value: 0.6867080745341615.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    study_name='depression_classification',\n",
    "    storage='sqlite:///optuna_depression.db',\n",
    "    load_if_exists=True,\n",
    "    pruner=optuna.pruners.MedianPruner(\n",
    "        n_startup_trials=5,\n",
    "        n_warmup_steps=10\n",
    "    )\n",
    ")\n",
    "\n",
    "n_trials = 50\n",
    "study.optimize(objective, n_trials=n_trials, n_jobs=4, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd4ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_trial.params\n",
    "\n",
    "n_layers = best_params['n_layers']\n",
    "hidden_dims = [best_params[f'hidden_dim_layer_{i}'] for i in range(n_layers)]\n",
    "\n",
    "best_model = FlexibleClassifier(\n",
    "    input_dim=19,\n",
    "    hidden_dims=hidden_dims,\n",
    "    output_dim=2,\n",
    "    dropout_rate=best_params['dropout_rate']\n",
    ")\n",
    "\n",
    "best_val_acc, _ = train_model(\n",
    "    model=best_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    lr=best_params['learning_rate'],\n",
    "    weight_decay=best_params['weight_decay'],\n",
    ")\n",
    "\n",
    "print(f\"  Best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaec0bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = best_model(torch.FloatTensor(x_test))\n",
    "    _, y_pred = torch.max(test_outputs, 1)\n",
    "\n",
    "target_names = ['Not Depressed', 'Depressed']\n",
    "print(classification_report(y_test, y_pred.numpy(), target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7024bdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred.numpy(), display_labels=target_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-advanced",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
