{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b9dd3c",
   "metadata": {},
   "source": [
    "Lecture: AI I - Advanced \n",
    "\n",
    "Previous:\n",
    "[**Chapter 4.2.1: Transformer with GPT2**](../02_nlp/01_gpt2.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519c7fe0",
   "metadata": {},
   "source": [
    "# Chapter 4.2.2: Text Classification & Sentiment Analysis\n",
    "\n",
    "In the Transformers excursion, you learned the inner workings of these models—how embeddings turn words into vectors, how attention lets every token attend to every other token, and how decoder-only models like GPT-2 generate text one token at a time. You even controlled GPT-2's output with temperature and sampling parameters.\n",
    "\n",
    "Now it's time to put a Transformer to work on a concrete task: text classification. Specifically, we'll train a model to decide whether a movie review is positive or negative—a task called sentiment analysis. This is one of the most fundamental NLP tasks and a perfect entry point into fine-tuning pre-trained language models.\n",
    "\n",
    "Instead of training a Transformer from scratch (which would require billions of examples and weeks of GPU time), we'll use a pre-trained model called DistilBERT and adapt it to our specific task. This approach—called fine-tuning—is how virtually all modern NLP applications are built.\n",
    "\n",
    "**Text classification** assigns a label to a piece of text. The label could be a sentiment (positive/negative), a topic (sports/politics/tech), a language, or anything else. What makes it a classification problem rather than a generation problem is that the output is one of a fixed set of categories—not free-form text.\n",
    "\n",
    "**Sentiment analysis** is classification applied to opinions. Given a movie review like \"The acting was brilliant but the plot made no sense\", the model should decide: is the overall sentiment positive or negative? This requires more than keyword matching. The model needs to understand sarcasm, weigh conflicting opinions, and grasp the overall tone—exactly the kind of contextual understanding that Transformer models excel at.\n",
    "\n",
    "## Why DistilBERT?\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) was a breakthrough: an encoder-only Transformer pre-trained on massive text corpora that could be fine-tuned for downstream tasks with remarkably little task-specific data. But BERT is large—110 million parameters for the base model—and slow at inference.\n",
    "\n",
    "DistilBERT is a distilled (compressed) version of BERT. It was created using knowledge distillation: a smaller \"student\" model was trained to mimic the behavior of the larger \"teacher\" model (BERT). The student doesn't need to learn from raw text—it learns from BERT's outputs, which are richer training signals than raw labels alone.\n",
    "\n",
    "The result: DistilBERT retains about 97% of BERT's performance on most tasks while being 40% smaller and 60% faster. For a course environment where GPU time matters, it's the ideal choice.\n",
    "\n",
    "We use `distilbert-base-cased`, the cased variant—meaning it distinguishes between \"The\" and \"the.\" Casing carries information (capitalized words at the start of sentences vs. proper nouns), so preserving it is slightly better for understanding meaning.\n",
    "\n",
    "## The IMDB Movie Reviews Dataset\n",
    "\n",
    "The IMDB dataset is hosted on HuggingFace and provides a clean, balanced split for supervised learning:\n",
    "\n",
    "| Split | Samples | Labels |\n",
    "|-------|---------|--------|\n",
    "| train | 25000   | 2 (1 = positive, 0 = negative) |\n",
    "| test  | 25000   | 2 (1 = positive, 0 = negative) |\n",
    "\n",
    "Each review is a plain-text string of variable length. Reviews are drawn from the polar ends of the rating scale — reviews with a score ≤ 4 are labelled negative, and those with a score ≥ 7 are labelled positive. Reviews scoring 5 or 6 are excluded entirely, which keeps the classification task clean and well-separated.\n",
    "\n",
    "The dataset is perfectly balanced — 12,500 positive and 12,500 negative reviews per split. Average review length is roughly 1,000 characters, though some reviews exceed 100,000 characters. This variation in length is important to consider when choosing tokenisation strategies.\n",
    "\n",
    "## Loading the Data\n",
    "\n",
    "We load the dataset using the HuggingFace datasets library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3516d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of training set: 25000\n",
      "length of test set: 25000\n",
      "example from training set: {'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "print(\"length of training set:\", len(dataset[\"train\"]))\n",
    "print(\"length of test set:\", len(dataset[\"test\"]))\n",
    "print(\"example from training set:\", dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c1347",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "DistilBERT doesn't work with raw text—it needs token IDs. The tokenizer converts text into a sequence of integer IDs that map to DistilBERT's vocabulary. It also handles special tokens and padding.\n",
    "\n",
    "DistilBERT uses WordPiece tokenization: words are split into subwords that exist in the model's vocabulary. Common words stay whole; rare words are broken into familiar pieces. The word \"unhappiness\" might become `[\"un\", \"##happiness\"]`—the `##` prefix indicates a continuation of the previous word.\n",
    "Two special tokens are always added:\n",
    "\n",
    "- `[CLS]` at the start: its final hidden state is used as the sentence-level representation (this is what we'll feed into our classification head)\n",
    "- `[SEP]` at the end: marks the boundary of the input\n",
    "\n",
    "### Max Length and Truncation\n",
    "\n",
    "DistilBERT has a maximum sequence length of 512 tokens. Reviews longer than this must be truncated. From our length analysis above, most reviews fit within this limit, but some don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0108f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0cdc02278c44a259f01109b74515456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf59aec0ccfb47be9800eaf3a75d66b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a928051514446589e36645239e6233a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased')\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"][:1000], truncation=True)\n",
    "\n",
    "tokenized_imdb = dataset.map(preprocess_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b44c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ffded2",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Fine-tuning pre-trained language models requires a much smaller learning rate than training from scratch. The pre-trained weights already encode rich linguistic knowledge—large gradient updates would destroy that knowledge. A learning rate of `2e-5` to `5e-5` is standard for BERT-family fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461d15e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-cased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./data/02_sentiment\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_imdb[\"train\"],\n",
    "    eval_dataset=tokenized_imdb[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3464ff25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Lecture: AI I - Advanced \n",
    "\n",
    "Next: [**Chapter 4.2.3: Named Entity Recognition**](../02_nlp/03_ner.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-advanced",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
