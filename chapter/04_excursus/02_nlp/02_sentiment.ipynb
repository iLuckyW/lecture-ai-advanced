{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b9dd3c",
   "metadata": {},
   "source": [
    "Lecture: AI I - Advanced \n",
    "\n",
    "Previous:\n",
    "[**Chapter 4.2.1: Transformer with GPT2**](../02_nlp/01_gpt2.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519c7fe0",
   "metadata": {},
   "source": [
    "# Chapter 4.2.2: Text Classification & Sentiment Analysis\n",
    "\n",
    "In the Transformers excursion, you learned the inner workings of these models—how embeddings turn words into vectors, how attention lets every token attend to every other token, and how decoder-only models like GPT-2 generate text one token at a time. You even controlled GPT-2's output with temperature and sampling parameters.\n",
    "\n",
    "Now it's time to put a Transformer to work on a concrete task: text classification. Specifically, we'll train a model to decide whether a movie review is positive or negative—a task called sentiment analysis. This is one of the most fundamental NLP tasks and a perfect entry point into fine-tuning pre-trained language models.\n",
    "\n",
    "Instead of training a Transformer from scratch (which would require billions of examples and weeks of GPU time), we'll use a pre-trained model called DistilBERT and adapt it to our specific task. This approach—called fine-tuning—is how virtually all modern NLP applications are built.\n",
    "\n",
    "**Text classification** assigns a label to a piece of text. The label could be a sentiment (positive/negative), a topic (sports/politics/tech), a language, or anything else. What makes it a classification problem rather than a generation problem is that the output is one of a fixed set of categories—not free-form text.\n",
    "\n",
    "**Sentiment analysis** is classification applied to opinions. Given a movie review like \"The acting was brilliant but the plot made no sense\", the model should decide: is the overall sentiment positive or negative? This requires more than keyword matching. The model needs to understand sarcasm, weigh conflicting opinions, and grasp the overall tone—exactly the kind of contextual understanding that Transformer models excel at.\n",
    "\n",
    "## Why DistilBERT?\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) was a breakthrough: an encoder-only Transformer pre-trained on massive text corpora that could be fine-tuned for downstream tasks with remarkably little task-specific data. But BERT is large—110 million parameters for the base model—and slow at inference.\n",
    "\n",
    "DistilBERT is a distilled (compressed) version of BERT. It was created using knowledge distillation: a smaller \"student\" model was trained to mimic the behavior of the larger \"teacher\" model (BERT). The student doesn't need to learn from raw text—it learns from BERT's outputs, which are richer training signals than raw labels alone.\n",
    "\n",
    "The result: DistilBERT retains about 97% of BERT's performance on most tasks while being 40% smaller and 60% faster. For a course environment where GPU time matters, it's the ideal choice.\n",
    "\n",
    "We use `distilbert-base-cased`, the cased variant—meaning it distinguishes between \"The\" and \"the.\" Casing carries information (capitalized words at the start of sentences vs. proper nouns), so preserving it is slightly better for understanding meaning.\n",
    "\n",
    "## The IMDB Movie Reviews Dataset\n",
    "\n",
    "The IMDB dataset is hosted on HuggingFace and provides a clean, balanced split for supervised learning:\n",
    "\n",
    "| Split | Samples | Labels |\n",
    "|-------|---------|--------|\n",
    "| train | 25000   | 2 (1 = positive, 0 = negative) |\n",
    "| test  | 25000   | 2 (1 = positive, 0 = negative) |\n",
    "\n",
    "Each review is a plain-text string of variable length. Reviews are drawn from the polar ends of the rating scale — reviews with a score ≤ 4 are labelled negative, and those with a score ≥ 7 are labelled positive. Reviews scoring 5 or 6 are excluded entirely, which keeps the classification task clean and well-separated.\n",
    "\n",
    "The dataset is perfectly balanced — 12,500 positive and 12,500 negative reviews per split. Average review length is roughly 1,000 characters, though some reviews exceed 100,000 characters. This variation in length is important to consider when choosing tokenisation strategies.\n",
    "\n",
    "## Loading the Data\n",
    "\n",
    "We load the dataset using the HuggingFace datasets library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3516d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of training set: 25000\n",
      "length of test set: 25000\n",
      "example from training set: {'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "print(\"length of training set:\", len(dataset[\"train\"]))\n",
    "print(\"length of test set:\", len(dataset[\"test\"]))\n",
    "print(\"example from training set:\", dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c1347",
   "metadata": {},
   "source": [
    "## Fine-Tuning a Pre-Trained DistilBERT Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0108f77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3464ff25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Lecture: AI I - Advanced \n",
    "\n",
    "Next: [**Chapter 4.2.3: Named Entity Recognition**](../02_nlp/03_ner.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-advanced",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
