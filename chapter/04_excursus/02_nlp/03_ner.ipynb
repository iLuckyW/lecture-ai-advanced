{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b9dd3c",
   "metadata": {},
   "source": [
    "Lecture: AI I - Advanced \n",
    "\n",
    "Previous:\n",
    "[**Chapter 4.2.2: Sentiment Analysis**](../02_nlp/02_sentiment.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519c7fe0",
   "metadata": {},
   "source": [
    "# Chapter 4.2.3: Named Entity Recognition\n",
    "\n",
    "In the previous chapter we trained DistilBERT to assign a single label to an entire movie review — positive or negative. That was **sequence classification**: one label per input.\n",
    "\n",
    "Named Entity Recognition (NER) is fundamentally different. Here every **token** in the sentence gets its own label. The task is to find and classify named entities — people, locations, organisations, and miscellaneous proper nouns — directly within the text. Given the sentence *\"Barack Obama visited Paris\"*, the model should output:\n",
    "\n",
    "| Token | Barack | Obama | visited | Paris |\n",
    "|---|---|---|---|---|\n",
    "| Label | B-PER | I-PER | O | B-LOC |\n",
    "\n",
    "This is called **token classification**: the same DistilBERT backbone is used, but instead of pooling down to a single `[CLS]` vector we take the hidden state of **every** token and pass each one through its own classification head. Architecturally the only change is swapping `AutoModelForSequenceClassification` for `AutoModelForTokenClassification`.\n",
    "\n",
    "## BIO Tagging\n",
    "\n",
    "Labels follow the **BIO (Begin–Inside–Outside)** scheme:\n",
    "- **B-** marks the **first** token of an entity (`B-PER` = beginning of a person name)\n",
    "- **I-** marks a **continuation** token of the same entity (`I-PER` = still inside a person name)\n",
    "- **O** means the token belongs to **no** entity\n",
    "\n",
    "The B/I distinction is essential when two entities of the same type appear next to each other — without it, *\"New York\"* (one location) and *\"New\"* + *\"York\"* (two separate locations) would be indistinguishable.\n",
    "\n",
    "## Why DistilBERT?\n",
    "\n",
    "Exactly the same reasoning as the sentiment chapter: DistilBERT retains ~97% of BERT's performance while being 40% smaller and 60% faster. We again use `distilbert-base-cased` — the cased variant is particularly important for NER because capitalisation is a strong signal. *\"Paris\"* (a city) and *\"paris\"* (unlikely to be a city) carry different information, and the cased model preserves that distinction.\n",
    "\n",
    "## The CoNLL-2003 Dataset\n",
    "\n",
    "| Split | Sentences | Entity types |\n",
    "|---|---|---|\n",
    "| train | 14 041 | PER, ORG, LOC, MISC |\n",
    "| validation | 3 250 | PER, ORG, LOC, MISC |\n",
    "| test | 3 684 | PER, ORG, LOC, MISC |\n",
    "\n",
    "Each example contains a `tokens` list and a `ner_tags` list of the same length. The 9 possible tag IDs are:\n",
    "\n",
    "| ID | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 |\n",
    "|---|---|---|---|---|---|---|---|---|---|\n",
    "| Tag | O | B-PER | I-PER | B-ORG | I-ORG | B-LOC | I-LOC | B-MISC | I-MISC |\n",
    "\n",
    "\n",
    "## Loading the Data\n",
    "\n",
    "We load the dataset using the HuggingFace datasets library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89259838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of training set: 14041\n",
      "length of validation set: 3250\n",
      "length of test set: 3453\n",
      "example from training set: {'id': '0', 'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0], 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"conll2003\", trust_remote_code=True)\n",
    "\n",
    "print(\"length of training set:\", len(dataset[\"train\"]))\n",
    "print(\"length of validation set:\", len(dataset[\"validation\"]))\n",
    "print(\"length of test set:\", len(dataset[\"test\"]))\n",
    "print(\"example from training set:\", dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caf72c8",
   "metadata": {},
   "source": [
    "## Tokenization & Label Alignment\n",
    "\n",
    "CoNLL-2003 arrives pre-tokenised into words, so we pass `is_split_into_words=True` to tell the tokeniser not to split on whitespace itself. DistilBERT then applies its own **WordPiece** subword splitting on top of that — the word *\"Washington\"* might become `[\"Was\", \"##hing\", \"##ton\"]`.\n",
    "\n",
    "This creates a mismatch: we have **one label per word**, but now **multiple subword tokens per word**. We resolve it with a simple rule:\n",
    "\n",
    "1. Use `word_ids()` to find out which original word each subword token came from.\n",
    "2. Assign the real label only to the **first** subword of each word.\n",
    "3. Set all other subwords — and special tokens like `[CLS]`/`[SEP]` — to **-100**, which `CrossEntropyLoss` ignores automatically.\n",
    "\n",
    "| | [CLS] | Was | ##hing | ##ton | visited | Paris | [SEP] |\n",
    "|---|---|---|---|---|---|---|---|\n",
    "| word\\_id | None | 0 | 0 | 0 | 1 | 2 | None |\n",
    "| label | -100 | B-LOC | -100 | -100 | O | B-LOC | -100 |\n",
    "\n",
    "We also switch from `DataCollatorWithPadding` to `DataCollatorForTokenClassification` — it pads both `input_ids` and `labels` to the same length within each batch, keeping the padding labels at -100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad88d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "\n",
    "def align_labels(examples):\n",
    "    \"\"\"Tokenize pre-split words and align NER labels with subword tokens.\"\"\"\n",
    "    tokenized = tokenizer(examples[\"tokens\"], is_split_into_words=True, truncation=True)\n",
    "\n",
    "    aligned_labels = []\n",
    "    for i, labels in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        previous_word_id = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_id != previous_word_id:\n",
    "                label_ids.append(labels[word_id])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "\n",
    "            previous_word_id = word_id\n",
    "        aligned_labels.append(label_ids)\n",
    "    tokenized[\"labels\"] = aligned_labels\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "tokenized_conll = dataset.map(align_labels, batched=True)\n",
    "data_collator   = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693cf1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "seqeval   = evaluate.load(\"seqeval\")\n",
    "label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    seqeval expects lists of string labels, not integers.\n",
    "    We filter out -100 (subword / special tokens) before scoring.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]  # noqa: E741\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (_, l) in zip(prediction, label) if l != -100]  # noqa: E741\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"precision\"],\n",
    "        \"recall\": results[\"recall\"],\n",
    "        \"f1\":results[\"f1\"],\n",
    "        \"accuracy\": results[\"accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cb6626",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The setup mirrors the sentiment chapter almost exactly. The only architectural change is `AutoModelForTokenClassification`: instead of a single linear layer on top of `[CLS]`, it applies a linear layer to **every** token's hidden state independently.\n",
    "\n",
    "`seqeval` evaluates at the **entity level**, not the token level. An entity is counted as correct only if *every* token in it is predicted correctly with the right B/I distinction. This is a stricter metric than per-token accuracy and is the standard for NER benchmarks. We therefore use `metric_for_best_model=\"f1\"` so the Trainer saves the checkpoint with the highest entity-level F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f509d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "id2label  = {i: name for i, name in enumerate(label_list)}\n",
    "label2id  = {name: i for i, name in enumerate(label_list)}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert-base-cased\",\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./data/03_ner\",\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_conll[\"train\"],\n",
    "    eval_dataset=tokenized_conll[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3464ff25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Lecture: AI I - Advanced \n",
    "\n",
    "Next: [**Chapter 4.2.4: AI Agents**](../02_nlp/04_agent.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-advanced",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
