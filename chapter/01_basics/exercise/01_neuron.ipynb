{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b9dd3c",
   "metadata": {},
   "source": [
    "Lecture: AI I - Advanced \n",
    "\n",
    "Previous:\n",
    "[**Chapter 1.1: Neuron**](../01_neuron.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519c7fe0",
   "metadata": {},
   "source": [
    "# Exercise 1.1: Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786ea8fd",
   "metadata": {},
   "source": [
    "> Hint: When doing the exercises put your solution in the designated \"Solution\" section:\n",
    "> ```python\n",
    "> # Solution (put your code here)\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aad877",
   "metadata": {},
   "source": [
    "## Task 1: Implement Activation Functions from Scratch\n",
    "\n",
    "Implement the core activation functions without using PyTorch's built-in functions.\n",
    "\n",
    "**Tasks**:\n",
    "- Implement ReLU, Sigmoid, Tanh, and Leaky ReLU\n",
    "- Test each function with a range of inputs\n",
    "- Verify your implementations match PyTorch's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "272369aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites (don't edit this block)\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7002705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (put your code here)\n",
    "def relu(x):\n",
    "    \"\"\"ReLU: max(0, x)\"\"\"\n",
    "    pass\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid: 1 / (1 + e^(-x))\"\"\"\n",
    "    pass\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"Tanh: (e^x - e^(-x)) / (e^x + e^(-x))\"\"\"\n",
    "    pass\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    \"\"\"Leaky ReLU: max(alpha*x, x)\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "221f0568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case (don't edit this block)\n",
    "x = torch.tensor([-3.0, -1.5, -0.5, 0.0, 0.5, 1.5, 3.0])\n",
    "\n",
    "assert torch.allclose(relu(x), torch.relu(x))\n",
    "assert torch.allclose(sigmoid(x), torch.sigmoid(x))\n",
    "assert torch.allclose(tanh(x), torch.tanh(x))\n",
    "assert torch.allclose(leaky_relu(x), torch.nn.functional.leaky_relu(x, negative_slope=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f41690",
   "metadata": {},
   "source": [
    "## Task 2: Build Logic Gates with Neurons\n",
    "\n",
    "Implement AND, OR, NAND, and NOR, gates using single neurons with manual weights.\n",
    "\n",
    "**Tasks**:\n",
    "- Design a 2-layer network with manual weights\n",
    "- Test all 4 input combinations\n",
    "- Explain how each hidden neuron contributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c711e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites (don't edit this block)\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa4f67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (put your code here)\n",
    "class LogicGate(nn.Module):\n",
    "    def __init__(self, gate_type):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 1)\n",
    "        self.gate_type = gate_type\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if gate_type == 'AND':  # Both inputs must be 1\n",
    "                pass\n",
    "            elif gate_type == 'OR':  # At least one input must be 1\n",
    "                pass\n",
    "            elif gate_type == 'NAND':  # NOT both inputs are 1\n",
    "                pass\n",
    "            elif gate_type == 'NOR':  # Neither input is 1\n",
    "                pass\n",
    "            \n",
    "            # Freeze weights\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f267b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3464ff25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Lecture: AI I - Advanced \n",
    "\n",
    "Next: [**Chapter 1.2: Multilayer Perceptron**](../02_mlp.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-advanced",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
