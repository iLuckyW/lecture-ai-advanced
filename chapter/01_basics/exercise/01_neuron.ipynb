{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b9dd3c",
   "metadata": {},
   "source": [
    "Lecture: AI I - Advanced \n",
    "\n",
    "Previous:\n",
    "[**Chapter 1.1: Neuron**](../01_neuron.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519c7fe0",
   "metadata": {},
   "source": [
    "# Exercise 1.1: Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786ea8fd",
   "metadata": {},
   "source": [
    "> Hint: When doing the exercises put your solution in the designated \"Solution\" section:\n",
    "> ```python\n",
    "> # Solution (put your code here)\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aad877",
   "metadata": {},
   "source": [
    "## Task 1: Implement Activation Functions from Scratch\n",
    "\n",
    "Implement the core activation functions without using PyTorch's built-in functions.\n",
    "\n",
    "**Tasks**:\n",
    "- Implement ReLU, Sigmoid, Tanh, and Leaky ReLU\n",
    "- Test each function with a range of inputs\n",
    "- Verify your implementations match PyTorch's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "272369aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites (don't edit this block)\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7002705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (put your code here)\n",
    "def relu(x):\n",
    "    \"\"\"ReLU: max(0, x)\"\"\"\n",
    "    pass\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid: 1 / (1 + e^(-x))\"\"\"\n",
    "    pass\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"Tanh: (e^x - e^(-x)) / (e^x + e^(-x))\"\"\"\n",
    "    pass\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    \"\"\"Leaky ReLU: max(alpha*x, x)\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "221f0568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case (don't edit this block)\n",
    "x = torch.tensor([-3.0, -1.5, -0.5, 0.0, 0.5, 1.5, 3.0])\n",
    "\n",
    "assert torch.allclose(relu(x), torch.relu(x))\n",
    "assert torch.allclose(sigmoid(x), torch.sigmoid(x))\n",
    "assert torch.allclose(tanh(x), torch.tanh(x))\n",
    "assert torch.allclose(leaky_relu(x), torch.nn.functional.leaky_relu(x, negative_slope=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f41690",
   "metadata": {},
   "source": [
    "## Task 2: Build Logic Gates with Neurons\n",
    "\n",
    "Implement AND, OR, NAND, and NOR, gates using single neurons with manual weights.\n",
    "\n",
    "**Tasks**:\n",
    "- Design a 2-layer network with manual weights\n",
    "- Test all 4 input combinations\n",
    "- Explain how each hidden neuron contributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c711e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites (don't edit this block)\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa4f67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (put your code here)\n",
    "class LogicGate(nn.Module):\n",
    "    def __init__(self, gate_type):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 1)\n",
    "        self.gate_type = gate_type\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if gate_type == 'AND':  # Both inputs must be 1\n",
    "                pass  # TODO\n",
    "            elif gate_type == 'OR':  # At least one input must be 1\n",
    "                pass  # TODO\n",
    "            elif gate_type == 'NAND':  # NOT both inputs are 1\n",
    "                pass  # TODO\n",
    "            elif gate_type == 'NOR':  # Neither input is 1\n",
    "                pass  # TODO\n",
    "            \n",
    "            # Freeze weights\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f267b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case (don't edit this block)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b872a09",
   "metadata": {},
   "source": [
    "## Task 3: Build XOR with Manual Weights \n",
    "\n",
    "Design a network that computes XOR using manually chosen weights, inspired by the absolute value MLP from the lecture.\n",
    "\n",
    "| A | B | XOR |\n",
    "|---|---|-----|\n",
    "| 0 | 0 |  0  |\n",
    "| 0 | 1 |  1  |\n",
    "| 1 | 0 |  1  |\n",
    "| 1 | 1 |  0  |\n",
    "\n",
    "Strategy: $XOR = (A \\text{ OR } B) \\text{ AND NOT}(A \\text{ AND } B) = (A \\text{ OR } B) \\text{ AND } (A \\text{ NAND } B)$\n",
    "\n",
    "**Tasks**:\n",
    "- Design a 2-layer network with manual weights\n",
    "- Test all 4 input combinations\n",
    "- Explain how each hidden neuron contributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dcd1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (put your code here)\n",
    "class LogicGateXOR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(2, 2)\n",
    "        self.output = nn.Linear(2, 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pass  # TODO\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92a0e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case (don't edit this block)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972014d1",
   "metadata": {},
   "source": [
    "## Task 4: Manual Forward Pass Through Multiple Neurons\n",
    "\n",
    "Given a network with manually set weights, compute outputs step-by-step.\n",
    "\n",
    "**Network Structure**:\n",
    "- Input: 2 features\n",
    "- Hidden layer: 3 neurons with ReLU\n",
    "- Output layer: 1 neuron with Sigmoid\n",
    "\n",
    "**Task**:\n",
    "- Manually compute the output for input [0.5, -1.0]\n",
    "- Show intermediate values (weighted sums, activations)\n",
    "- Implement and verify with code\n",
    "\n",
    "```\n",
    "Hidden Layer:\n",
    "  Neuron 1: W = [1.0, -0.5], b = 0.2\n",
    "  Neuron 2: W = [-1.0, 2.0], b = -0.3\n",
    "  Neuron 3: W = [0.5, 0.5], b = 0.0\n",
    "\n",
    "Output Layer:\n",
    "  Neuron 1: W = [1.0, -0.5, 2.0], b = 0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459c31b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (put your code here)\n",
    "class ManualNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(2, 3)\n",
    "        self.output = nn.Linear(3, 1)\n",
    "        \n",
    "        # Set weights manually\n",
    "        with torch.no_grad():\n",
    "            pass  # TODO\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17ec4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case (don't edit this block)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3464ff25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Lecture: AI I - Advanced \n",
    "\n",
    "Next: [**Chapter 1.2: Multilayer Perceptron**](../02_mlp.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-advanced",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
