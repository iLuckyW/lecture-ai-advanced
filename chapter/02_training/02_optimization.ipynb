{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b9dd3c",
   "metadata": {},
   "source": [
    "Lecture: AI I - Advanced \n",
    "\n",
    "Previous:\n",
    "[**Chapter 2.1: Regularization**](../02_training/01_regularization.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519c7fe0",
   "metadata": {},
   "source": [
    "# Chapter 2.2: Hyperparameter Tuning\n",
    "\n",
    "In the previous section, you learned various regularization techniques to prevent overfitting—dropout, batch normalization, early stopping, and weight decay. But each technique introduced new hyperparameters: What dropout rate should you use? How much weight decay? You probably chose values like 0.3 or 0.01 somewhat arbitrarily, perhaps based on common defaults. Now comes the question: How do you find the optimal configuration among thousands of possible combinations?\n",
    "\n",
    "This is where **hyperparameter tuning** becomes essential. Instead of guessing or manually trying different values, we'll use systematic search algorithms to explore the hyperparameter space efficiently and find configurations that maximize our model's performance.\n",
    "\n",
    "## What Are Hyperparameters?\n",
    "\n",
    "**Hyperparameters** are configuration settings that you choose before training begins—they control the learning process itself, unlike model parameters (weights and biases) which are learned during training. Choosing good hyperparameters can mean the difference between a model that achieves 70% accuracy and one that reaches 95%.\n",
    "\n",
    "## Hyperparameters can include:\n",
    "### 1. Optimizer and Learning Rate\n",
    "\n",
    "The **optimizer** determines how weights are updated (SGD, Adam, RMSprop), while the **learning rate** controls the step size. This is often the most critical hyperparameter—too high and training diverges, too low and learning is painfully slow. Learning rates typically range from $1e-5$ to $1e-1$ and are best searched on a logarithmic scale (since 0.001 to 0.01 is as significant a change as 0.01 to 0.1).\n",
    "\n",
    "**Search strategy**: Log-uniform distribution <br>\n",
    "**Typical range**: 1e-5 to 1e-1\n",
    "\n",
    "### 2. Loss Function\n",
    "\n",
    "For classification, you'll typically use CrossEntropyLoss, while regression uses MSELoss or L1Loss. The choice depends on your task rather than being a tunable parameter, though in some cases (e.g., imbalanced classification) you might compare weighted vs. unweighted loss functions.\n",
    "\n",
    "**Search strategy**: Usually fixed by task, occasionally categorical choice\n",
    "\n",
    "### 3. Epochs\n",
    "\n",
    "The number of training iterations through the entire dataset. Too few epochs and your model underfits; too many and you waste time or overfit (though early stopping mitigates this). Modern practice often uses a high max_epochs with early stopping rather than tuning this directly.\n",
    "\n",
    "**Search strategy**: Usually fixed with early stopping, occasionally discrete <br>  \n",
    "**Typical range**: 50-500\n",
    "\n",
    "### 4. Batch Size\n",
    "\n",
    "How many samples are processed before updating weights. Smaller batches (16-32) add noise that can help generalization but slow training; larger batches (128-256) are faster but may converge to sharper minima. Batch size also interacts with learning rate—larger batches often need higher learning rates.\n",
    "\n",
    "**Search strategy**: Categorical choice (powers of 2) <br>\n",
    "**Typical values**: [16, 32, 64, 128]\n",
    "\n",
    "### 6. Per-Layer Configuration\n",
    "\n",
    "Each layer can have different settings:\n",
    "- **Activation function**: ReLU is standard, but Leaky ReLU, ELU, or GELU might work better for specific tasks\n",
    "- **Neuron count (hidden dimensions)**: Typically powers of 2 (32, 64, 128, 256, 512) for computational efficiency\n",
    "- **Regularization parameters** (dropout rate, initialization) can also vary per layer\n",
    "\n",
    "**Search strategy**: Categorical (activation) or discrete/categorical (neuron count)\n",
    "\n",
    "### 7. Architecture\n",
    "\n",
    "**Depth** (number of layers) and **shape** (how dimensions change between layers—pyramid, constant, hourglass):\n",
    "- **Shallow networks (1-2 hidden layers)**: Simpler, less prone to overfitting, limited capacity\n",
    "- **Deep networks (3+ hidden layers)**: More expressive, can learn hierarchical features, harder to train\n",
    "\n",
    "**Search strategy**: Discrete (number of layers) + continuous/discrete (dimension pattern)\n",
    "\n",
    "## Search Strategy\n",
    "\n",
    "**Random search** tries random combinations—simple but inefficient. Grid search is systematic but exponentially expensive (3 values for 5 hyperparameters = 3^5 = 243 trials!).\n",
    "\n",
    "**Bayesian optimization** is smarter: it builds a probabilistic model of how hyperparameters affect performance, then uses this model to suggest promising configurations. After each trial, it updates its belief about which regions of the hyperparameter space are worth exploring. This means it finds good configurations in far fewer trials than random or grid search.\n",
    "\n",
    "**Optuna** implements several Bayesian optimization algorithms, with **Tree-structured Parzen Estimator (TPE)** as the default. Think of it as learning from experience: \"High learning rates with small batch sizes performed poorly, so let's try lower learning rates next.\"\n",
    "\n",
    "## Hyperparameter Tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27c87661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "torch.manual_seed(42)  # set random seed for reproducibility\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "x = iris.data  # 4 features: sepal length, sepal width, petal length, petal width\n",
    "y = iris.target  # 3 classes: setosa, versicolor, virginica\n",
    "\n",
    "x_scaled = StandardScaler().fit_transform(x)\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(\n",
    "    x_scaled, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "x_val, x_test, y_val, y_test = train_test_split(\n",
    "    x_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(TensorDataset(torch.FloatTensor(x_train), torch.LongTensor(y_train)), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(torch.FloatTensor(x_val), torch.LongTensor(y_val)), batch_size=batch_size)\n",
    "test_loader = DataLoader(TensorDataset(torch.FloatTensor(x_test), torch.LongTensor(y_test)), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d216949a",
   "metadata": {},
   "source": [
    "### Flexible Model Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eeb5109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FlexibleClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Flexible neural network that can be configured with different architectures.\n",
    "    Supports variable depth and layer dimensions.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Number of input features\n",
    "            hidden_dims: List of hidden layer dimensions, e.g., [64, 32]\n",
    "            output_dim: Number of output classes\n",
    "            dropout_rate: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Build layers dynamically\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d55eb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        train_loss += loss.item() * batch_X.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "    \n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, criterion):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    eval_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in data_loader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            eval_loss += loss.item() * batch_X.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "    \n",
    "    avg_loss = eval_loss / len(data_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=100, lr=0.01, weight_decay=0.0, patience=15, trial=None):\n",
    "    \"\"\"Complete training loop\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer) # Train\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion)  # Validate\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "    \n",
    "        if trial is not None:\n",
    "            trial.report(val_acc, epoch)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "    \n",
    "    return best_val_acc, best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b35315",
   "metadata": {},
   "source": [
    "### Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "245d489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to optimize.\n",
    "    Returns the best validation accuracy for this hyperparameter configuration.\n",
    "    \"\"\"\n",
    "    # Define the search space\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    hidden_dims = []\n",
    "    for i in range(n_layers):\n",
    "        dim = trial.suggest_categorical(f'hidden_dim_layer_{i}', [32, 64, 128, 256])\n",
    "        hidden_dims.append(dim)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-2, log=True)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)    \n",
    "    \n",
    "    # Training the model with the sampled hyperparameters\n",
    "    model = FlexibleClassifier(input_dim=4,hidden_dims=hidden_dims,output_dim=3,dropout_rate=dropout_rate)        \n",
    "    best_val_acc, best_epoch = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        trial=trial  # For Optuna pruning\n",
    "    )\n",
    "    \n",
    "    return best_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce20e55e",
   "metadata": {},
   "source": [
    "### Run Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "471922eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-01 12:22:58,393] A new study created in memory with name: iris_classification\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09723c76df947dbb0e89f17e8f8bb5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2026-01-01 12:23:00,134] Trial 0 finished with value: 0.6666666666666666 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 128, 'hidden_dim_layer_1': 32, 'hidden_dim_layer_2': 128, 'weight_decay': 0.00017521385419341513, 'learning_rate': 1.5014294676988676e-05, 'dropout_rate': 0.15151730828178306}. Best is trial 0 with value: 0.6666666666666666.\n",
      "[I 2026-01-01 12:23:00,232] Trial 1 finished with value: 0.8666666666666667 and parameters: {'n_layers': 2, 'hidden_dim_layer_0': 32, 'hidden_dim_layer_1': 64, 'weight_decay': 1.2968865790865804e-05, 'learning_rate': 7.389379112502169e-05, 'dropout_rate': 0.1183884609885662}. Best is trial 1 with value: 0.8666666666666667.\n",
      "[I 2026-01-01 12:23:00,351] Trial 2 finished with value: 0.7666666666666667 and parameters: {'n_layers': 2, 'hidden_dim_layer_0': 256, 'hidden_dim_layer_1': 64, 'weight_decay': 0.0027702092908288923, 'learning_rate': 0.00017215312095741434, 'dropout_rate': 0.24284535046588496}. Best is trial 1 with value: 0.8666666666666667.\n",
      "[I 2026-01-01 12:23:00,547] Trial 3 finished with value: 0.9666666666666667 and parameters: {'n_layers': 2, 'hidden_dim_layer_0': 256, 'hidden_dim_layer_1': 128, 'weight_decay': 0.0025930075887686324, 'learning_rate': 0.0005763804435094203, 'dropout_rate': 0.12503650676188016}. Best is trial 3 with value: 0.9666666666666667.\n",
      "[I 2026-01-01 12:23:00,692] Trial 4 finished with value: 0.9666666666666667 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 32, 'hidden_dim_layer_1': 128, 'hidden_dim_layer_2': 32, 'weight_decay': 0.005972721791214056, 'learning_rate': 0.007402867745799745, 'dropout_rate': 0.038465780114957104}. Best is trial 3 with value: 0.9666666666666667.\n",
      "[I 2026-01-01 12:23:00,781] Trial 5 finished with value: 0.9666666666666667 and parameters: {'n_layers': 1, 'hidden_dim_layer_0': 64, 'weight_decay': 0.0008351923676558726, 'learning_rate': 0.020523594919081196, 'dropout_rate': 0.04746022512946402}. Best is trial 3 with value: 0.9666666666666667.\n",
      "[I 2026-01-01 12:23:00,817] Trial 6 pruned. \n",
      "[I 2026-01-01 12:23:00,936] Trial 7 pruned. \n",
      "[I 2026-01-01 12:23:01,139] Trial 8 finished with value: 0.9666666666666667 and parameters: {'n_layers': 2, 'hidden_dim_layer_0': 256, 'hidden_dim_layer_1': 128, 'weight_decay': 0.008284212581527802, 'learning_rate': 0.001358329853075775, 'dropout_rate': 0.49780791488149057}. Best is trial 3 with value: 0.9666666666666667.\n",
      "[I 2026-01-01 12:23:01,199] Trial 9 pruned. \n",
      "[I 2026-01-01 12:23:01,299] Trial 10 finished with value: 0.9666666666666667 and parameters: {'n_layers': 1, 'hidden_dim_layer_0': 128, 'weight_decay': 0.0014225187429805018, 'learning_rate': 0.06606234165798826, 'dropout_rate': 0.33649939861139005}. Best is trial 3 with value: 0.9666666666666667.\n",
      "[I 2026-01-01 12:23:01,483] Trial 11 finished with value: 1.0 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 32, 'hidden_dim_layer_1': 128, 'hidden_dim_layer_2': 32, 'weight_decay': 0.003147103809275303, 'learning_rate': 0.00659712784993868, 'dropout_rate': 0.01379391858893363}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:01,616] Trial 12 finished with value: 0.9333333333333333 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 32, 'hidden_dim_layer_1': 128, 'hidden_dim_layer_2': 64, 'weight_decay': 0.00011963059355195549, 'learning_rate': 0.006682245159635998, 'dropout_rate': 0.005918567146308191}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:01,782] Trial 13 finished with value: 1.0 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 256, 'hidden_dim_layer_1': 128, 'hidden_dim_layer_2': 32, 'weight_decay': 0.0019151874187681452, 'learning_rate': 0.08136650859901619, 'dropout_rate': 0.09287935728016028}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:01,957] Trial 14 finished with value: 1.0 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 256, 'hidden_dim_layer_1': 128, 'hidden_dim_layer_2': 32, 'weight_decay': 0.0009341654422538893, 'learning_rate': 0.09979007424532498, 'dropout_rate': 0.07114640824935373}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:02,129] Trial 15 finished with value: 1.0 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 32, 'hidden_dim_layer_1': 32, 'hidden_dim_layer_2': 32, 'weight_decay': 0.0003850324284255504, 'learning_rate': 0.0251409964222858, 'dropout_rate': 0.3296784509970911}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:02,289] Trial 16 finished with value: 0.9666666666666667 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 128, 'hidden_dim_layer_1': 256, 'hidden_dim_layer_2': 256, 'weight_decay': 0.0031252312869899444, 'learning_rate': 0.024125650875947514, 'dropout_rate': 0.006557060139757672}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:02,442] Trial 17 finished with value: 0.9666666666666667 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 64, 'hidden_dim_layer_1': 128, 'hidden_dim_layer_2': 32, 'weight_decay': 0.0011344116725482518, 'learning_rate': 0.004500531815282424, 'dropout_rate': 0.1819545837760676}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:02,608] Trial 18 finished with value: 1.0 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 256, 'hidden_dim_layer_1': 128, 'hidden_dim_layer_2': 32, 'weight_decay': 7.642848388325528e-05, 'learning_rate': 0.03747947128470314, 'dropout_rate': 0.09053593173611724}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:02,837] Trial 19 finished with value: 1.0 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 32, 'hidden_dim_layer_1': 128, 'hidden_dim_layer_2': 64, 'weight_decay': 0.004289035670687221, 'learning_rate': 0.011433525231461537, 'dropout_rate': 0.3057530515898037}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:03,005] Trial 20 finished with value: 1.0 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 256, 'hidden_dim_layer_1': 256, 'hidden_dim_layer_2': 256, 'weight_decay': 0.0014541930763836792, 'learning_rate': 0.0032066235426456697, 'dropout_rate': 0.2773934406861778}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:03,155] Trial 21 finished with value: 0.9666666666666667 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 256, 'hidden_dim_layer_1': 128, 'hidden_dim_layer_2': 32, 'weight_decay': 0.0006022000850735636, 'learning_rate': 0.08940111341292863, 'dropout_rate': 0.07394167102846873}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:03,235] Trial 22 pruned. \n",
      "[I 2026-01-01 12:23:03,410] Trial 23 finished with value: 1.0 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 256, 'hidden_dim_layer_1': 128, 'hidden_dim_layer_2': 32, 'weight_decay': 0.0008011085856356136, 'learning_rate': 0.048885194624373164, 'dropout_rate': 0.09570130517500774}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:03,554] Trial 24 finished with value: 1.0 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 256, 'hidden_dim_layer_1': 32, 'hidden_dim_layer_2': 128, 'weight_decay': 0.0002448003819686971, 'learning_rate': 0.012975494902623513, 'dropout_rate': 0.002464364675020439}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:03,666] Trial 25 finished with value: 1.0 and parameters: {'n_layers': 2, 'hidden_dim_layer_0': 32, 'hidden_dim_layer_1': 128, 'weight_decay': 0.0018845515212259176, 'learning_rate': 0.041489295833737456, 'dropout_rate': 0.1626227173661598}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:03,836] Trial 26 finished with value: 0.9666666666666667 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 128, 'hidden_dim_layer_1': 128, 'hidden_dim_layer_2': 32, 'weight_decay': 0.004826152226427393, 'learning_rate': 0.0017885343502365945, 'dropout_rate': 0.20453350024083272}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:04,009] Trial 27 finished with value: 1.0 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 64, 'hidden_dim_layer_1': 128, 'hidden_dim_layer_2': 32, 'weight_decay': 0.0006544772403205953, 'learning_rate': 0.015032390034929361, 'dropout_rate': 0.04702263861443205}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:04,155] Trial 28 finished with value: 1.0 and parameters: {'n_layers': 2, 'hidden_dim_layer_0': 256, 'hidden_dim_layer_1': 128, 'weight_decay': 0.0024999116358795917, 'learning_rate': 0.0409223618165961, 'dropout_rate': 0.09956729064884794}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:04,222] Trial 29 pruned. \n",
      "[I 2026-01-01 12:23:04,293] Trial 30 pruned. \n",
      "[I 2026-01-01 12:23:04,412] Trial 31 finished with value: 0.9666666666666667 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 32, 'hidden_dim_layer_1': 32, 'hidden_dim_layer_2': 32, 'weight_decay': 0.00033716472752570427, 'learning_rate': 0.026022684685260023, 'dropout_rate': 0.4067927759538328}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:04,603] Trial 32 finished with value: 1.0 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 32, 'hidden_dim_layer_1': 32, 'hidden_dim_layer_2': 32, 'weight_decay': 0.00045844790966027745, 'learning_rate': 0.06110122547318991, 'dropout_rate': 0.35550592456794017}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:04,769] Trial 33 finished with value: 1.0 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 32, 'hidden_dim_layer_1': 32, 'hidden_dim_layer_2': 32, 'weight_decay': 0.0001077577568825973, 'learning_rate': 0.008675918464360869, 'dropout_rate': 0.24382262554419873}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:04,919] Trial 34 finished with value: 1.0 and parameters: {'n_layers': 2, 'hidden_dim_layer_0': 32, 'hidden_dim_layer_1': 32, 'weight_decay': 0.003760582260618269, 'learning_rate': 0.021613935572238237, 'dropout_rate': 0.34857815398754766}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:05,070] Trial 35 finished with value: 1.0 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 32, 'hidden_dim_layer_1': 64, 'hidden_dim_layer_2': 32, 'weight_decay': 0.0019036351194210643, 'learning_rate': 0.09490190060397051, 'dropout_rate': 0.2811792084786011}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:05,206] Trial 36 finished with value: 1.0 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 256, 'hidden_dim_layer_1': 32, 'hidden_dim_layer_2': 256, 'weight_decay': 0.0057544101451850235, 'learning_rate': 0.03522185269672117, 'dropout_rate': 0.11995347634689259}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:05,333] Trial 37 pruned. \n",
      "[I 2026-01-01 12:23:05,387] Trial 38 pruned. \n",
      "[I 2026-01-01 12:23:05,497] Trial 39 finished with value: 0.9666666666666667 and parameters: {'n_layers': 2, 'hidden_dim_layer_0': 64, 'hidden_dim_layer_1': 128, 'weight_decay': 5.684119913440201e-05, 'learning_rate': 0.006212244405084911, 'dropout_rate': 0.20644558126083423}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:05,622] Trial 40 finished with value: 1.0 and parameters: {'n_layers': 2, 'hidden_dim_layer_0': 256, 'hidden_dim_layer_1': 128, 'weight_decay': 0.000473214965605092, 'learning_rate': 0.01909264083948811, 'dropout_rate': 0.07467678617799471}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:05,845] Trial 41 finished with value: 1.0 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 256, 'hidden_dim_layer_1': 128, 'hidden_dim_layer_2': 32, 'weight_decay': 4.900229082427388e-05, 'learning_rate': 0.032999694366363665, 'dropout_rate': 0.09723663051642474}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:06,005] Trial 42 finished with value: 0.9666666666666667 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 256, 'hidden_dim_layer_1': 128, 'hidden_dim_layer_2': 32, 'weight_decay': 0.00012067934384123711, 'learning_rate': 0.06251664368024609, 'dropout_rate': 0.1660050829139474}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:06,150] Trial 43 finished with value: 1.0 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 256, 'hidden_dim_layer_1': 128, 'hidden_dim_layer_2': 32, 'weight_decay': 0.00017687906677045907, 'learning_rate': 0.05374672251874442, 'dropout_rate': 0.07661978097970645}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:06,312] Trial 44 finished with value: 0.9666666666666667 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 256, 'hidden_dim_layer_1': 128, 'hidden_dim_layer_2': 32, 'weight_decay': 6.798044670101016e-05, 'learning_rate': 0.010185839167346994, 'dropout_rate': 0.4620267232411024}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:06,501] Trial 45 finished with value: 1.0 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 32, 'hidden_dim_layer_1': 64, 'hidden_dim_layer_2': 32, 'weight_decay': 0.0008795889977752173, 'learning_rate': 0.015590480513217679, 'dropout_rate': 0.04886241709024641}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:06,578] Trial 46 pruned. \n",
      "[I 2026-01-01 12:23:06,783] Trial 47 finished with value: 1.0 and parameters: {'n_layers': 3, 'hidden_dim_layer_0': 128, 'hidden_dim_layer_1': 256, 'hidden_dim_layer_2': 32, 'weight_decay': 0.00030778386359990275, 'learning_rate': 0.026832181504047623, 'dropout_rate': 0.02245245967470813}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:06,865] Trial 48 finished with value: 1.0 and parameters: {'n_layers': 1, 'hidden_dim_layer_0': 64, 'weight_decay': 0.007812502150236968, 'learning_rate': 0.06670410675455318, 'dropout_rate': 0.10647567481909317}. Best is trial 11 with value: 1.0.\n",
      "[I 2026-01-01 12:23:06,933] Trial 49 pruned. \n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    study_name='iris_classification',\n",
    "    pruner=optuna.pruners.MedianPruner(\n",
    "        n_startup_trials=5,\n",
    "        n_warmup_steps=10\n",
    "    )\n",
    ")\n",
    "\n",
    "n_trials = 50\n",
    "study.optimize(objective, n_trials=n_trials, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5107e5fa",
   "metadata": {},
   "source": [
    "### Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2816ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3464ff25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Lecture: AI I - Basics \n",
    "\n",
    "Exercise: [**Exercise 2.2: Hyperparameter Tuning**](../02_training/exercises/02_optimization.ipynb)\n",
    "\n",
    "Next: [**Chapter 2.3: Ensemble Learning**](../02_training/03_ensemble.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-advanced",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
