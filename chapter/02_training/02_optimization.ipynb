{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b9dd3c",
   "metadata": {},
   "source": [
    "Lecture: AI I - Advanced \n",
    "\n",
    "Previous:\n",
    "[**Chapter 2.1: Regularization**](../02_training/01_regularization.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519c7fe0",
   "metadata": {},
   "source": [
    "# Chapter 2.2: Hyperparameter Tuning\n",
    "\n",
    "In the previous section, you learned various regularization techniques to prevent overfitting—dropout, batch normalization, early stopping, and weight decay. But each technique introduced new hyperparameters: What dropout rate should you use? How much weight decay? You probably chose values like 0.3 or 0.01 somewhat arbitrarily, perhaps based on common defaults. Now comes the question: How do you find the optimal configuration among thousands of possible combinations?\n",
    "\n",
    "This is where **hyperparameter tuning** becomes essential. Instead of guessing or manually trying different values, we'll use systematic search algorithms to explore the hyperparameter space efficiently and find configurations that maximize our model's performance.\n",
    "\n",
    "## What Are Hyperparameters?\n",
    "\n",
    "**Hyperparameters** are configuration settings that you choose before training begins—they control the learning process itself, unlike model parameters (weights and biases) which are learned during training. Choosing good hyperparameters can mean the difference between a model that achieves 70% accuracy and one that reaches 95%.\n",
    "\n",
    "## Hyperparameters can include:\n",
    "### 1. Optimizer and Learning Rate\n",
    "\n",
    "The **optimizer** determines how weights are updated (SGD, Adam, RMSprop), while the **learning rate** controls the step size. This is often the most critical hyperparameter—too high and training diverges, too low and learning is painfully slow. Learning rates typically range from $1e-5$ to $1e-1$ and are best searched on a logarithmic scale (since 0.001 to 0.01 is as significant a change as 0.01 to 0.1).\n",
    "\n",
    "**Search strategy**: Log-uniform distribution <br>\n",
    "**Typical range**: 1e-5 to 1e-1\n",
    "\n",
    "### 2. Loss Function\n",
    "\n",
    "For classification, you'll typically use CrossEntropyLoss, while regression uses MSELoss or L1Loss. The choice depends on your task rather than being a tunable parameter, though in some cases (e.g., imbalanced classification) you might compare weighted vs. unweighted loss functions.\n",
    "\n",
    "**Search strategy**: Usually fixed by task, occasionally categorical choice\n",
    "\n",
    "### 3. Epochs\n",
    "\n",
    "The number of training iterations through the entire dataset. Too few epochs and your model underfits; too many and you waste time or overfit (though early stopping mitigates this). Modern practice often uses a high max_epochs with early stopping rather than tuning this directly.\n",
    "\n",
    "**Search strategy**: Usually fixed with early stopping, occasionally discrete <br>  \n",
    "**Typical range**: 50-500\n",
    "\n",
    "### 4. Batch Size\n",
    "\n",
    "How many samples are processed before updating weights. Smaller batches (16-32) add noise that can help generalization but slow training; larger batches (128-256) are faster but may converge to sharper minima. Batch size also interacts with learning rate—larger batches often need higher learning rates.\n",
    "\n",
    "**Search strategy**: Categorical choice (powers of 2) <br>\n",
    "**Typical values**: [16, 32, 64, 128]\n",
    "\n",
    "### 6. Per-Layer Configuration\n",
    "\n",
    "Each layer can have different settings:\n",
    "- **Activation function**: ReLU is standard, but Leaky ReLU, ELU, or GELU might work better for specific tasks\n",
    "- **Neuron count (hidden dimensions)**: Typically powers of 2 (32, 64, 128, 256, 512) for computational efficiency\n",
    "- **Regularization parameters** (dropout rate, initialization) can also vary per layer\n",
    "\n",
    "**Search strategy**: Categorical (activation) or discrete/categorical (neuron count)\n",
    "\n",
    "### 7. Architecture\n",
    "\n",
    "**Depth** (number of layers) and **shape** (how dimensions change between layers—pyramid, constant, hourglass):\n",
    "- **Shallow networks (1-2 hidden layers)**: Simpler, less prone to overfitting, limited capacity\n",
    "- **Deep networks (3+ hidden layers)**: More expressive, can learn hierarchical features, harder to train\n",
    "\n",
    "**Search strategy**: Discrete (number of layers) + continuous/discrete (dimension pattern)\n",
    "\n",
    "## Search Strategy\n",
    "\n",
    "**Random search** tries random combinations—simple but inefficient. Grid search is systematic but exponentially expensive (3 values for 5 hyperparameters = 3^5 = 243 trials!).\n",
    "\n",
    "**Bayesian optimization** is smarter: it builds a probabilistic model of how hyperparameters affect performance, then uses this model to suggest promising configurations. After each trial, it updates its belief about which regions of the hyperparameter space are worth exploring. This means it finds good configurations in far fewer trials than random or grid search.\n",
    "\n",
    "**Optuna** implements several Bayesian optimization algorithms, with **Tree-structured Parzen Estimator (TPE)** as the default. Think of it as learning from experience: \"High learning rates with small batch sizes performed poorly, so let's try lower learning rates next.\"\n",
    "\n",
    "## Hyperparameter Tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c87661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "torch.manual_seed(42)  # set random seed for reproducibility\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "x = iris.data  # 4 features: sepal length, sepal width, petal length, petal width\n",
    "y = iris.target  # 3 classes: setosa, versicolor, virginica\n",
    "\n",
    "x_scaled = StandardScaler().fit_transform(x)\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(\n",
    "    x_scaled, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "x_val, x_test, y_val, y_test = train_test_split(\n",
    "    x_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(TensorDataset(torch.FloatTensor(x_train), torch.LongTensor(y_train)), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(torch.FloatTensor(x_val), torch.LongTensor(y_val)), batch_size=batch_size)\n",
    "test_loader = DataLoader(TensorDataset(torch.FloatTensor(x_test), torch.LongTensor(y_test)), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d216949a",
   "metadata": {},
   "source": [
    "### Flexible Model Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeb5109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FlexibleClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Flexible neural network that can be configured with different architectures.\n",
    "    Supports variable depth and layer dimensions.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Number of input features\n",
    "            hidden_dims: List of hidden layer dimensions, e.g., [64, 32]\n",
    "            output_dim: Number of output classes\n",
    "            dropout_rate: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Build layers dynamically\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3464ff25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Lecture: AI I - Basics \n",
    "\n",
    "Exercise: [**Exercise 2.2: Hyperparameter Tuning**](../02_training/exercises/02_optimization.ipynb)\n",
    "\n",
    "Next: [**Chapter 2.3: Ensemble Learning**](../02_training/03_ensemble.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-advanced",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
